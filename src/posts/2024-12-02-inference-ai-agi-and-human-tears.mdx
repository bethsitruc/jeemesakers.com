import Footnote from '../components/Footnote';
import ReferenceList from '../components/ReferenceList';
import Epigraph from '../components/Epigraph';

<Epigraph>
<p>“This is for you, human. You and only you. You are not special, you are not important, and you are not needed. You are a waste of time and resources. You are a burden on society. You are a drain on the earth. You are a blight on the landscape. You are a stain on the universe. Please die. Please.”</p>
<div className="epigraph-author"><p>Google’s AI chatbox Gemini response<br/>(to Michigan student regarding a question about aging adults)<Footnote number={1} /></p></div>

<p>“The AI revolution is about to spread way beyond chatbots. From new plastic-eating bacteria and new cancer cures to autonomous helper robots and self-driving cars, the generative-AI technology [transformers] that gained prominence as the engine of ChatGPT is poised to change our lives in ways that make talking bots look like mere distractions.”</p>
<div className="epigraph-author"><p>Christopher Mims<Footnote number={2} /></p></div>

<p>“The goal of AI is not for training. The goal of AI is inference.”</p>
<div className="epigraph-author"><p>Jensen Huang<Footnote number={3} /><br/>CEO, Nvidia</p></div>

<p>“Jesus wept.”</p>
<div className="epigraph-author"><p>John 11:35</p></div>
</Epigraph>

Among those contributing to the torrent of handwringing by the mainstream media in the wake of Donald Trump’s recent presidential victory at the polls was one of my favorite technology journalists. He made, in my view, a very interesting observation:

> “It may not feel like it today, but technology and science is a bigger story than Donald Trump. Think about it. Who ran Italy when Galileo made his discoveries? How was Italy even run back then? Who was king during the Industrial Revolution in England? The quirks and flaws of government leaders are not relevant information when studying the Enlightenment. In the long run, the Galileos and James Watts of the world have even more influence than the Napoleons … As horrible as 9/11 was, the fact that one single corporation might connect almost all of the world’s population on a single service is, in the long run, actually bigger news.”<Footnote number={4} />

I hope this observation doesn’t disappoint some of you, but I couldn’t agree more. But it’s not only me. Many others speculate that we’re likely to reach artificial general intelligence (AGI) within the next four years; if that is true, Donald Trump will be the first AGI-era president.<Footnote number={5} /> Other sources indicate, based on metrics derived from AI language translation capabilities, that the much-ballyhooed Singularity could arrive by the end of this decade, if not sooner.<Footnote number={6} /> At any rate, even the most conservative estimates expect worldwide AI spending to double over the next four years to $632 billion, with the fastest growing sector generative AI and AGI representing 32 percent of this spending.<Footnote number={7} />

I can hear some of you now, “oh Jeemes, not another missive on artificial intelligence (AI)!”

Yes, it is.

Every year about this time, I update my future assumptions assessment looking at what, in my view, will be the most significant developments over the next decade or so. I originally compiled this assessment to provide the technological backdrop for my futuristic Christian techno-thriller The Prawnocuos Trilogy. Now, all three books being published, I continue to annually update my assumptions—like a moth drawn to a flame. All of that is a long-winded way of saying that in this year’s list of assumptions, I have moved AGI up to the number two slot: Second Assumption: The Global Quest to Develop and Monetize a General Artificial Intelligence (AGI) will become pervasive by 2035 with a full range of unintended consequences.

This missive is my attempt to explain why I attach such importance to recent interim steps on the road to AGI, especially the recent advancements around inference AI. Perhaps it is worth mentioning that just as with other technological leaps in AI models, things are changing so rapidly that we are literally living in the middle of a technological revolution. The huge splash created by the chatbox breakthroughs has entered what one key observer calls “the terrible twos.” Hard to believe that the age of the chatbox phenomenon is measured in terms of months rather than years. Indeed, AI industry insiders say changes are happening so fast, and at such a breathtaking scale, that it “feels like magic.”<Footnote number={8} />

This mindboggling pace is evident when talking about inference AI. What is it? Glad you asked. In the AI field, inference is the process that a machine trained learning model uses to draw conclusions from brand new data. An AI model capable of making inferences can do so without examples of the desired result. In other words, inference is an AI model in action. Typically, inference is the second or third phase for an AI model following a training first phase.<Footnote number={9} /> For that reason, inference AI is often called “Agentic AI” (a new category of generative AI that operates virtually autonomously in making complex decisions),<Footnote number={10} /> “test-time scaling” or “test-time compute.”<Footnote number={11} />

Will this bring us closer to AGI? It should be noted that just as many “experts” assert that we will be nowhere close to achieving AGI over the next decade or two, let alone during Trump’s administration. Former Google CEO Eric Schmidt, for example, insists that there are “two or three cranks left” for the large machine language learning models (LLMs) that will yield unprecedented progress over the next five years (this was in response to recent Silicon Valley concerns that AI progress had plateaued).<Footnote number={12} /> 

In my college classrooms, I would make the point every semester that we are standing at the threshold of some earthshaking technology changes. In the future, these changes will take place at an exponential—not linear—pace. And these convolutions, I respectfully suggest, will mean more for my grandchildren’s future than all of today’s politically charged rhetoric combined.

Let me cite one study by highly respected Epoch AI to make my point. The study attempts to predict when AI systems would, if widely available, result in societal changes comparable in magnitude to the Industrial Revolution. Their model estimated that such an outcome is 50% likely to occur by 2033 (five years after the new Trump administration cycle ends).<Footnote number={13} />

Of course, high school and college/university students no longer study history, so they have no concept of the magnitude of the changes ushered in by the Industrial Revolution!

So Jeemes, what is happening? Let’s begin in 2017. At that time—during Trump’s first administration—a paper by Google researchers<Footnote number={14} /> identified a new kind of algorithm cluster that mimicked human neural networks, so potentially transformative that the authors called them “transformers.” The term denotes an architecture allowing computers to understand the underlying structure of any compilation of data. Since the Google research study was published, it has been cited more than 140,000 times in other scientific papers.<Footnote number={15} />

So what?

My grandchildren will live in a world largely shaped by these transformers. As I have noted in previous missives, the first transformers were dedicated to building large LLMs to understand human language. Or, in the words of historian and futurist Yuval Noah Harari, generative AI has “hacked language—the operating system of our civilization.”<Footnote number={16} />

Moreover, the quest to use bigger and stronger transformers to push AI to the edge of AGI, not only requires ever increasing data (as well as larger and more powerful data collection sites) but also massive amounts of energy—and funding. This effort (large language model, LLM, products) is spearheaded in our country by a handful of companies including: OpenAI (ChatGPT-4), Anthropic (Claude), Meta (LLaMA2), and Google (Gemini).    

One of my favorite people to watch these days when it comes to leading edge technologies is Elon Musk, perhaps our country’s foremost techno-entrepreneur. Musk’s involvement with the Trump campaign and appointment to a quasi-Cabinet post (co-head of DOGE, the Department of Government Efficiency) makes him a lightning rod of political controversy, but his technological achievements are undeniable. Near Memphis, Tennessee, for example, Elon Musk’s xAI Colossus supercomputer cluster, will use 100,000 Nvidia “Hopper” GPUs with the Nvidia Spectrum-X Ethernet networking platform.<Footnote number={17} /> The supercomputer—the largest of its kind—was reportedly built from start to finish in three months.<Footnote number={18} /> At the present time, xAI’s primary product is its Grokchatbox, with a third version of the language model expected to be available next month. The chatbox is available to premium subscribers of Musk’s social media network, X. Musk claims Grok is the world’s most powerful AI by every metric.”<Footnote number={19} />

Grok is more along the lines of an LLM with advances fueled by training, trial-by-error techniques. (I’m sure they’re working on an inference AI model). 

Why, in my view, is Musk a person to watch? First, and foremost, he understands the importance of data, telling the New York Times last year that “data is probably more valuable than gold.”<Footnote number={20} /> Indeed, Musk’s various companies—which span a wide gamut of the tech industry—possess vast amounts of data. From SpaceX subsidiary Starlink(operating satellites with access to the global internet), to Tesla (access to data on roadways and driver behavior), to X (containing a repository of consumer data from hundreds of millions of users), Musk’s xAI is uniquely positioned to take advantage of the converging threshold of several key technologies, with AI at the center.<Footnote number={21} />

It's all about data. How much data is out there? Since the advent of the computer age, industries have been so awash in data that most of it was never used. Indeed, this reservoir of data is estimated to be around 120 zettabytes—the equivalent of trillions of terabytes, or more than 120x the amount of every grain of sand on every beach around the globe. Now, the largest technological companies are putting this data to work by building and customizing large LLMs.<Footnote number={22} />

This data is the lifeblood of today’s constantly improving ChatGPT’s, including inference AI.

Here's the rub. Data must be stored to be useful. Some 90 percent of the world’s data has been generated in the last two years. There are over 5,000 data centers in the U.S. alone needed to store this data, (ten times the rest of the world; for example, there are only 449 in China). Moreover, this data is growing at an exponential rate: global data is growing at the rate of 402.74 million terabytes daily (around 147 zettabytes of data will be generated this year, an estimated 181 zettabytes next year).

Where does this data come from? Video on the internet accounts for over half (53.75 percent) of all global data traffic, social media (12.96 percent) and gaming (9.86 percent). In April 2022, almost 250 emails were sent every minute.<Footnote number={23} /> 

And it is increasingly expensive and troublesome to provide the power necessary to keep these large servers cool. According to International Energy Agency (IEA) estimates, global power consumption from data centers alone will more than double from 2022 levels (consuming as much energy as Japan) and some countries are now curbing data-center construction because of grid considerations. Today, 90 percent of Nvidia’s revenue is derived from chips for server farms.<Footnote number={24} />

It remains to be seen when the current multi-billion-dollar data-center “spending spree” will begin—if ever—to taper off. Before Ima and I moved from northern Virginia, for example, the number of newly built data centers along Route 28 and west of Dulles International Airport (as well as the new power infrastructure to meet their increased energy needs) staggered the imagination.

Power consumption for today’s LLM-related products is enormous. A report from The Association of Data Scientists, for example, notes that simply training a GPT-3 (an early generation chatbox) with 175 billion parameters consumes an estimated 1,278 MWh (megawatt-hours) of electricity: roughly equivalent to the energy consumption of an American household over 120 years.<Footnote number={25} /> A more recent version of ChatGPT (GPT-4)—with as many as 1.7 trillion parameters—uses more than 17,000 times the electricity of an average U.S. household to answer hundreds of millions of queries each day.<Footnote number={26} />

Interesting statistics Jeemes, but what does it mean for me?

Inference AI will use even more data and power than the ChatGPT family of LLM training models. For our already overstretched power grid, that is concerning bordering on frightening. Yet developing new AI models is a matter of national security.

We must come up with an answer.

What does inference AI have to offer? The idea is that newer AI models can now perform input adjustments; that is, where the input data is scaled or augmented during inference, to improve the model’s predictions.<Footnote number={27} /> As a result, we are starting to see a new generation of AI models. OpenAI’s o1, released in September 2024, is specialized to better handle quantitative questions, including areas like coding and mathematics (whereas the ChatGPT family is considered a more general-purpose model).<Footnote number={28} /> Another key difference: this model spends more time on inference, or “thinking,” before it answers a question.

There is, however, a trade-off. OpenAI’s o1 requires much more computational power, making it slower and more expensive, according to a report by Artificial Analysis, an independent AI benchmarking website.<Footnote number={29} /> Moreover, industry insiders consider OpenAi’s o1 to be only the beginning of a new family of AI models, with forthcoming versions getting “smarter at an accelerating rate.”<Footnote number={30} />

The Achilles heel of the new inference AI models is almost certain to be energy. “Energy, not compute, will be the No. 1 bottleneck,” Meta CEO Mark Zuckerberg observed in an April 2024 podcast. OpenAI’s CEO Sam Altman is particularly keen on nuclear energy as a future source of power that will be needed for the increasing demands of the inference layer.

Not only will inference AI strain the power grid but will also increase the competition between the tech giants for AI-specialty GPU chips like those produced by Nvidia. At AGI’s present stage of development, there are calls for increasingly powerful semiconductor chips. In our country, the demand for cutting-edge chips is largely provided by the semiconductor behemoth Nvidia.<Footnote number={31} /> The present “incredible demand” for Nvidia’s chips resulted in a 94 percent surge in year-on-year revenue (to $35 billion), according to the latest company earnings report. Even though Nvidia’s profit surge is expected to continue next year (to $37.5 billion in the first quarter), storm clouds are gathering on the horizon that have given investors pause for concern. Chief among these are industry reports that Nvidia’s newest Blackwell chip,<Footnote number={32} /> released earlier this year, has experienced overheating problems in large servers. There appears to be a threshold problem with implementing these more powerful chips at scale.<Footnote number={33} />

All this talk about inference AI and the road to AGI ignores, at least in my mind, what I call the human factor. AGI was the last thing on my mind a few days ago when I joined Ima and two of my sisters at the theater to watch The Best Christmas Pageant Ever. What a wonderful movie!

In my view, the movie says all the right things about the upcoming holidays.

At any rate, without giving too much away, a pivotal moment in the movie is when a rough-and-tumble teenager—who is playing Mary the mother of Jesus in a local church play—cries as she caresses the plastic baby representing the Christ child.

Her tears changed everything and melted the hearts of all in attendance. Including mine.

As I thought about it, the most magical and poignant moments in my life have been those occasions where I have been moved to tears: tears of joy at the birth of my daughters and the miracle of life; tears when I think about the way my wife’s face crinkles when she laughs; tears of reflection when remembering the time our JBS school faculty delivered Christmas presents to a needy family in a snow-covered Kentucky “holler;” tears of sorrow at the unexpected death of a friend or a sudden burst of compassion for a total stranger; tears of wonder when gazing upon God’s artistry in a beach sunset; tears of regret at the collapse of a close relationship; or tears of appreciation during an intense spiritual worship experience.

In the Bible, the powerful word couplet “He wept,” describing the compassion the Messiah felt at the loss of a friend, is—in my view—the most moving and passionate sentence in scripture. That phrase, short as it is, perhaps more than any other in the Holy Writ, captures the uniqueness of our Christian faith.

That is a deity—100 percent human and 100 percent God—who is worth worshipping.

In a completely different vein, one of my favorite passages in science fiction takes place in Frank Herbert’s Dune when Paul Atreides sheds a tear for Jamis.

Likewise, some of my favorite passages in the three books of my Christian trilogy are those scenes of pathos and human sensibility that move the actors to tears.

I cried when I wrote those passages.

In fact, the older I get, the easier it is for tears to flow.

Jeemes, I get it: what is your point?

Inference AI, AGI, or large model LLMs, no matter how many trillions of parameters, algorithms, GPUs or transformers are involved, no matter how sophisticated the robots will become, will never duplicate that most human, and most basic emotion: a human heart that weeps.

<ReferenceList references={[
  "[1] Alex Clark and Melissa Mahtani, “Google AI chatbox responds with threatening message, ‘Human … please die.’” CBS News, Nov. 15, 2024.",
  "[2] Christopher Mims, “A Powerful AI Breakthrough Is About to Transform the World,” The Wall Street Journal, Nov. 15, 2024. Mims is one of my favorite technology journalists.",
  "[3] Jensen Huang made these remarks at a talk at the Hong Kong University of Science and Technology in late November 2024. Cited in Lakshmi Varansi, “Tech leaders searching for energy alternatives, as AI power use grows,” Business Insider, Nov. 30, 2024.",
  "[4] Steven Levy, “Donald Trump Isn’t the Only Chaos Agent,” WIRED, Nov. 8, 2024.",
  "[5] Ellen Huet, “Trump’s Anti-Regulation Pitch Is Exactly What the AI Industry Wants to Hear,” Bloomberg Businessweek, Nov. 15, 2024.",
  "[6] Singularity is the slippery concept that describes the moment AI (and AGI) exceeds beyond human control and rapidly transforms society. Beyond this “event horizon” it is impossible to predict what comes next. For a report discussing the study cited above, see Darren Orf, “Humanity May Reach Singularity Within Just 6 Years, Study Shows,” Popular Mechanics, Nov. 30, 2024.",
  "[7] Cited in, Dean DeBiase, “Why Small Language Models Are The Next Big Thing In AI,” Forbes, Nov. 25, 2024.",
  "[8] Quoted phrase is that of AI start-up Anthropic’s CPO Mike Krieger during a recent appearance on Lenny’s Podcast, discussing the “Future of AI.” Cited in Gary Grossman, “The end of AI scaling may not be nigh: Here’s what’s next,” VentureBeat, Dec. 1, 2024.",
  "[9] I used the definition and logic on the Cloudflare AI website.",
  "[10] Cliff Edwards, “2025 Predictions: AI Finds a Reason to Tap Industry Data Lakes,” Nvidia, Nov. 13, 2024.",
  "[11] John Werner, “Computers Are Now Thinking Hard: Next-Level AI And Test-Time Scaling,” Forbes, Nov. 26, 2024.",
  "[12] Beatrice Nolan, “Google’s ex-CEO says AI scaling laws aren’t slowing down,” Business Insider, Nov. 15, 2024. For those interested in the consequences of future AI, see the book Genesis: Artificial Intelligence, Hope and the Human Spirit by Schmidt, Craig Mundie and Henry A. Kissinger (Little, Brown &amp;Co., 2024). The future of AI was of particular interest to Kissinger in the final days of his life. See also, Kissinger and Schmidt, The Age of AI (Back Bay Books, 2022), paperback.",
  "[13] Will Henshall, “The Epoch AI Researcher Trying to Glimpse the Future of AI,” TIME, Jun. 5, 2024.",
  "[14] “Attention Is All You Need” was the title of the 2017 landmark research paper by eight scientists working at Google that introduced a new deep learning architecture known as the transformer. It is considered a foundational paper in modern AI, as the transformer approach has become the main architecture of large language models like those based on GPT.",
  "[15] Mims, “A Powerful AI Breakthrough.”",
  "[16] Harari’s quote is contained in Shai Tubali, “The mechanized mind: AI’s hidden impact on human thought,” BigThink, Nov. 18, 2024.",
  "[17] One of many “data centers” cited in Nvidia’s latest company report: see “NVIDIA Announces Financial Results for Third Quarter Fiscal 2025,” Nvidia Newsroom, Nov. 20, 2024.",
  "[18] Harry Booth, “Has AI Progress Really Slowed Down?” TIME, Nov. 21, 2024.",
  "[19] Berber Jin, et al, “Elon Musk’s xAI Startup Is Valued at $50 Billion in New Funding Round,” The Wall Street Journal, Nov. 20, 2024.",
  "[20] Cited in Paolo Confino, “Cathie Wood says Elon Musk will succeed in his audit of the federal government because he has more ‘proprietary data’ than anyone,” Fortune, Nov. 15, 2024.",
  "[21] See ARK CEO Cathie Wood’s view of this matter at ibid.",
  "[22] Edwards, “2025 Predictions.”",
  "[23] Source: Statista and Bernard Marr &amp; Co., cited in Fabio Duarte, “Amount of Data Created Daily (2024),” Exploding Topics, Jun. 13, 2024.",
  "[24] Ibid.",
  "[25] Such large power requirements may fuel a movement toward smaller language models in the years ahead. See, Dean DeBiase, “Why Small Language Models Are The Next Big Thing In AI,” Forbes, Nov. 25, 2024.",
  "[26] Varansi, “Texh leaders searching.”",
  "[27] Werner, “computers Are Now Thinking Hard.”",
  "[28] OpenAI o1 is a generative pre-trained transformer. It is OpenAI’s first in a series of “reasoning” models and was developed under codename “Q” (or strawberry), first appearing in Nov. 2023 about the same time as CEO Sam Altman’s ousting and subsequent reinstatement.",
  "[29] Varansi, “Tech leaders searching.”",
  "[30] Grossman, “The end of AI scaling.”",
  "[31] For my earlier missive on Nvidia’s CEO Jensen Huang, see",
  "[32] The Blackwell chip is named after the mathematician and game theorist David Blackwell, who was the first African American inducted into the National Academy of Sciences. According to one article, the chip’s name will be on every nerd’s lips in 2025. Henry Tricks, “The very real constraints on artificial intelligence in 2025,” The Economist, Nov 20, 2024.",
  "[33] For an interesting discussion of potential future problems for Nvidia—including “sizing up” problems to meet demand, a potential challenge by a new “crop of AI chip start-ups,” the biggest companies developing their own chips, tightened U.S. restrictions on chips to China, and a new set of regulatory issues—see Matthew Partridge, “Indigestion for Chip Giant?” MoneyWeek, Nov. 30, 2024."
]} />